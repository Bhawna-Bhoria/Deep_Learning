# -*- coding: utf-8 -*-
"""M22MA003_Minor2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p0SVhA8Qr90ZtM_tdd0d3fHZX9K7PTn2
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader
# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
import os



# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime as dt
import torch.nn.functional as F
import torch
from torch import optim, nn
from torch.utils.data import DataLoader, TensorDataset, Dataset
from torchvision.utils import make_grid
from torchvision import transforms as T
from torchvision import models, datasets
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator
# from ignite.metrics import Accuracy, Loss, Precision, Recall
# from ignite.handlers import LRScheduler, ModelCheckpoint, global_step_from_engine
# from ignite.contrib.handlers import ProgressBar, TensorboardLogger
# import ignite.contrib.engines.common as common

# import opendatasets as od
import os
from random import randint
import urllib
import zipfile



!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
  
# Unzip raw zip file
!unzip -qq 'tiny-imagenet-200.zip'

# Define main data directory
DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]

# Define training and validation data paths
TRAIN_DIR = os.path.join(DATA_DIR, 'train') 
VALID_DIR = os.path.join(DATA_DIR, 'val')
val_img_dir = os.path.join(VALID_DIR, 'images')

# Open and read val annotations text file
fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')
data = fp.readlines()

# Create dictionary to store img filename (word 0) and corresponding
# label (word 1) for every line in the txt file (as key value pair)
val_img_dict = {}
for line in data:
    words = line.split('\t')
    val_img_dict[words[0]] = words[1]
fp.close()

# Display first 10 entries of resulting val_img_dict dictionary
{k: val_img_dict[k] for k in list(val_img_dict)[:10]}
for img, folder in val_img_dict.items():
    newpath = (os.path.join(val_img_dir, folder))
    if not os.path.exists(newpath):
        os.makedirs(newpath)
    if os.path.exists(os.path.join(val_img_dir, img)):
        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))
preprocess_transform_pretrain = T.Compose([
                T.Resize(64), # Resize images to 64 x 64
                T.ToTensor(),  # Converting cropped images to tensors
                T.Normalize(mean=[0.485, 0.456, 0.406], 
                            std=[0.229, 0.224, 0.225])
])
batch_size = 50
# Create DataLoaders for pre-trained models (normalized based on specific requirements)
train_dataset = datasets.ImageFolder(TRAIN_DIR, transform = preprocess_transform_pretrain)
test_dataset = datasets.ImageFolder(val_img_dir, transform = preprocess_transform_pretrain)
train_loader = DataLoader(train_dataset, batch_size = batch_size, 
                              shuffle = True, drop_last = True)

test_loader = DataLoader(test_dataset, batch_size = batch_size, 
                              drop_last = True)

train_iter = iter(train_loader)
x_train, y_train = next(train_iter)
# samples, labels = next(iter(train_loader))
# print(samples.shape, labels.shape, type(samples))
# for i in range(6):
#   plt.subplot(2,3,i+1)
#   plt.imshow(samples[i][0])
#   plt.show

dataiter = iter(train_loader)
images, labels = next(dataiter)
def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()
# show images
imshow(torchvision.utils.make_grid(images))

num_epochs = 8
batch_size = 50
learning_rate = 0.001

x_train.shape

import random
import time
import torch 
import torch.nn as nn
import torch.nn.functional as F



class Teacher(nn.Module):
    def __init__(self):
        super(Teacher, self).__init__()
        self.conv1 = nn.Conv2d(3, 12, 3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(12, 14, 3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(14, 14, 3, stride=1, padding=1)
        self.conv4 = nn.Conv2d(14, 14, 3, stride=1, padding=1)
        self.conv5 = nn.Conv2d(14, 14, 3, stride=1, padding=1)
        self.conv6 = nn.Conv2d(14, 14, 3, stride=1, padding=1)
        self.conv7 = nn.Conv2d(14, 18, 3, stride=1, padding=1)
        self.conv8 = nn.Conv2d(18, 18, 5, stride=1, padding=1)
        self.conv9 = nn.Conv2d(18, 18, 5, stride=1, padding=2)
        self.conv10 = nn.Conv2d(18, 18, 5, stride=1, padding=2)
        self.conv11 = nn.Conv2d(18, 18, 5, stride=1, padding=2)
        self.conv12 = nn.Conv2d(18, 18, 5, stride=1, padding=2)
        self.pool = nn.AvgPool2d(2, 2)
        # self.fc=nn.Linear(10*16*16,512)
        self.fc=nn.Linear(18*31*31,512)
        self.fc1 = nn.Linear(512, 200)
        torch.nn.init.xavier_normal_(self.conv1.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv2.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv3.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv4.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv5.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv6.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv7.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv8.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv9.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv10.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv11.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv12.weight, gain=1.0)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        # print(x.shape)
        x = F.relu(self.conv2(x))
        # print(x.shape)
        x = F.relu(self.conv3(x))
        # print(x.shape)
        x = F.relu(self.conv4(x))
        # print(x.shape)
        x = F.relu(self.conv5(x))
        # print(x.shape)
        x = F.relu(self.conv6(x))
        # print(x.shape)
        x = F.relu(self.conv7(x))
        # print(x.shape)
        x = F.relu(self.conv8(x))
        # print(x.shape)
        x = F.relu(self.conv9(x))
        # print(x.shape)
        x = F.relu(self.conv10(x))
        # print(x.shape)
        x = F.relu(self.conv11(x))
        # print(x.shape)
        x = F.relu(self.conv12(x))
        # print(x.shape)
        x = self.pool(x)
        # print(x.shape)
        # x = x.view(-1, 10 * 16 * 16)
        x = x.view(-1, 18*31*31)
        # print(x.shape)
        x = F.relu(self.fc(x))
        # print(x.shape)
        x = F.relu(self.fc1(x))
        # x = x.view(-1, 2 * 16 * 16)
        # print(x.shape)
        # x = F.relu(self.fc(x))
        # print(x.shape)
        # x = F.relu(self.fc1(x))
        # print(x.shape)
        return x



if __name__ == "__main__":
    img = torch.randn(2, 3, 64, 64)
    t = Teacher()
    # s = Student(3, 32, 10, 0.2)
    start = time.time()
    out_1, out_2 = t(img)
    print(f"time taken for teacher = {time.time() - start}")



teacher_model = Teacher().to(device)
alpha=0.5
loss_criterion = nn.CrossEntropyLoss()
teacher_optimizer = torch.optim.SGD(teacher_model.parameters(), lr=learning_rate)

n_total_steps = len(train_loader)

print(n_total_steps)


def train_method(num_epochs,data_loader,loss_criterion,optimizer,model):
    n_correct=0
    # t_loss=0
    loss=0
    for i, (images, labels) in enumerate(data_loader):

        images = images.to(device)
        labels = labels.to(device)
        
        # Forward pass
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        t_loss = loss_criterion(outputs, labels)
        loss += t_loss
        n_correct += (predicted == labels).sum().item()
        # Backward and optimize
        optimizer.zero_grad()
        t_loss.backward()
        optimizer.step()
        # print(i)
        if (i+1) % 500 == 0:
            acc = (100.0 * n_correct )/ (batch_size*(i+1))
            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Train Accuracy: {acc:.3f} , Train Loss: {t_loss.item():.4f}')
    loss /= len(data_loader)
    PATH = './cnn.pth'
    torch.save(model.state_dict(), PATH)
    return acc,loss

def test_method(num_epochs,data_loader,model,loss_criterion):
    with torch.no_grad():
        n_correct = 0
        n_samples = 0
        loss=0
        for images, labels in data_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            t_loss = loss_criterion(outputs, labels)
            loss +=t_loss
            # max returns (value ,index)
            _, predicted = torch.max(outputs, 1)
            n_samples += labels.size(0)
            n_correct += (predicted == labels).sum().item()
        acc = (100.0 * n_correct )/ (len(data_loader)*batch_size)
        final_loss=loss/len(data_loader)
        print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy of the network: {acc} %, Test Loss: {final_loss:.4f}')
        print('------------------------------------------------------------------------------------------------------------------------------------')
        return acc,final_loss


train_loss_list = []
test_loss_list = []
train_accuracy_list = []
test_accuracy_list = []
for epoch in range(num_epochs):
  train_acc,train_loss = train_method(num_epochs,train_loader,loss_criterion,teacher_optimizer,teacher_model)
  test_acc, test_loss = test_method(num_epochs,test_loader,teacher_model,loss_criterion)
  print(train_acc,train_loss, test_acc, test_loss)
  train_loss_list.append(train_loss.item())
  test_loss_list.append(test_loss.item())
  train_accuracy_list.append(train_acc)
  test_accuracy_list.append(test_acc)
print('Finished Training Teacher Network')

## loss vs epoch plot
def loss_plot(train, test):
  plt.figure(figsize = (10,7))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.plot(range(len(train)),train_loss_list, label = "Train Loss")
  plt.plot(range(len(test)),test_loss_list, label = "Test Loss")
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.title("Loss")
  plt.legend()
  plt.show()
## Accuracy vs epoch plot
def accuracy_plot(train, test):
  plt.figure(figsize = (10,7))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.plot(range(len(train)),train_accuracy_list, label = "Train Accuracy")
  plt.plot(range(len(test)),test_accuracy_list, label = "Test Accuracy")
  plt.xlabel("Epochs")
  plt.ylabel("Accuracy")
  plt.title("Accuracy")
  plt.legend()
  plt.show()

loss_plot(train_loss_list, test_loss_list)
print('  ')
accuracy_plot(train_accuracy_list, test_accuracy_list)



import random
import time
import torch 
import torch.nn as nn
import torch.nn.functional as F



class Student(nn.Module):
    def __init__(self):
        super(Student, self).__init__()
        self.conv1 = nn.Conv2d(3, 12, 3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(12, 10, 3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(10, 10, 3, stride=1, padding=1)
        self.conv4 = nn.Conv2d(10, 8, 5, stride=1, padding=1)
        self.pool = nn.AvgPool2d(2, 2)
        # self.fc=nn.Linear(10*16*16,512)
        self.fc=nn.Linear(8*31*31,512)
        self.fc1 = nn.Linear(512, 200)
        torch.nn.init.xavier_normal_(self.conv1.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv2.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv3.weight, gain=1.0)
        torch.nn.init.xavier_normal_(self.conv4.weight, gain=1.0)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        # print(x.shape)
        x = F.relu(self.conv2(x))
        # print(x.shape)
        x = F.relu(self.conv3(x))
        # print(x.shape)
        x = F.relu(self.conv4(x))
        # print(x.shape)
        x = self.pool(x)
        # print(x.shape)
        # x = x.view(-1, 10 * 16 * 16)
        x = x.view(-1, 8*31*31)
        # print(x.shape)
        x = F.relu(self.fc(x))
        # print(x.shape)
        x = F.relu(self.fc1(x))
        # x = x.view(-1, 2 * 16 * 16)
        # # print(x.shape)
        # x = F.relu(self.fc(x))
        # # print(x.shape)
        # x = F.relu(self.fc1(x))
        return x



if __name__ == "__main__":
    img = torch.randn(2, 3, 64, 64)
    t = Student()
    start = time.time()
    out_1, out_2 = t(img)
    print(f"time taken for teacher = {time.time() - start}")

# criterion = nn.CrossEntropyLoss()
# loss = criterion(output / temperature, target) + temperature ** 2 * distillation_loss(teacher_model(data), output, temperature, criterion)

# def distillation_loss(teacher_output, student_output, temperature, criterion):
#     soft_teacher_output = nn.functional.softmax(teacher_output / temperature, dim=1)
#     soft_student_output = nn.functional.softmax(student_output / temperature, dim=1)
#     return temperature ** 2 * criterion(soft_student_output, soft_teacher_output.detach())

temperature = 3
def t_n_s_loss_fn(
    t_out,
    s_out, 
    labels, 
):
    cce_l = nn.CrossEntropyLoss()(s_out, labels.view(-1))
    s_out = nn.LogSoftmax()(s_out/temperature)
    t_out = torch.softmax(t_out/temperature, dim=-1)
    kde_loss = nn.KLDivLoss()(s_out, t_out)
    
    return kde_loss*(0.3*temperature*temperature) + cce_l*0.7

st_model = Student().to(device)

st_loss_criterion = nn.CrossEntropyLoss()

st_optimizer = torch.optim.SGD(st_model.parameters(), lr=learning_rate)

st_n_total_steps = len(train_loader)

print(st_n_total_steps)


def student_train_method(num_epochs,data_loader,loss_criterion,optimizer,model):
    st_n_correct=0
    # t_loss=0
    loss=0
    for i, (images, labels) in enumerate(data_loader):

        images = images.to(device)
        labels = labels.to(device)
        # Forward pass
        t_out = teacher_model(images)
        s_out = st_model(images)
        _, predicted = torch.max(s_out, 1)
        t_loss = t_n_s_loss_fn(t_out,s_out, labels)
        loss += t_loss
        st_n_correct += (predicted == labels).sum().item()
        # Backward and optimize
        st_optimizer.zero_grad()
        t_loss.backward()
        st_optimizer.step()
        # print(i)
        if (i+1) % 500 == 0:
            acc = (100.0 * st_n_correct )/ (batch_size*(i+1))
            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Train Accuracy: {acc:.3f} , Train Loss: {t_loss.item():.4f}')
    loss /= len(data_loader)
    PATH = './cnn.pth'
    torch.save(model.state_dict(), PATH)
    return acc,loss

def student_test_method(num_epochs,data_loader,model,loss_criterion):
    with torch.no_grad():
        n_correct = 0
        n_samples = 0
        loss=0
        for images, labels in data_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            t_loss = loss_criterion(outputs, labels)
            loss +=t_loss
            # max returns (value ,index)
            _, predicted = torch.max(outputs, 1)
            n_samples += labels.size(0)
            n_correct += (predicted == labels).sum().item()
        acc = (100.0 * n_correct )/ (len(data_loader)*batch_size)
        final_loss=loss/len(data_loader)
        print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy of the network: {acc} %, Test Loss: {final_loss:.4f}')
        print('------------------------------------------------------------------------------------------------------------------------------------')
        return acc,final_loss


student_train_loss_list = []
student_test_loss_list = []
student_train_accuracy_list = []
student_test_accuracy_list = []
for epoch in range(num_epochs):
  train_acc,train_loss = student_train_method(num_epochs,train_loader,st_loss_criterion,teacher_optimizer,st_model)
  test_acc, test_loss = student_test_method(num_epochs,test_loader,st_model,st_loss_criterion)
  print(train_acc,train_loss, test_acc, test_loss)
  student_train_loss_list.append(train_loss.item())
  student_test_loss_list.append(test_loss.item())
  student_train_accuracy_list.append(train_acc)
  student_test_accuracy_list.append(test_acc)
print('Finished Training Student Network')

## loss vs epoch plot
def loss_plot(train, test):
  plt.figure(figsize = (10,7))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.plot(range(len(train)),student_train_loss_list, label = "Train Loss")
  plt.plot(range(len(test)),student_test_loss_list, label = "Test Loss")
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.title("Loss")
  plt.legend()
  plt.show()
## Accuracy vs epoch plot
def accuracy_plot(train, test):
  plt.figure(figsize = (10,7))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.plot(range(len(train)),student_train_accuracy_list, label = "Train Accuracy")
  plt.plot(range(len(test)),student_test_accuracy_list, label = "Test Accuracy")
  plt.xlabel("Epochs")
  plt.ylabel("Accuracy")
  plt.title("Accuracy")
  plt.legend()
  plt.show()



# loss_plot(student_train_loss_list, student_test_loss_list)
print('  ')
accuracy_plot(student_train_accuracy_list, student_test_accuracy_list)

st_ema_model = Student().to(device)
ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged: 0.1 * averaged_model_parameter + 0.9 * model_parameter
ema_model = torch.optim.swa_utils.AveragedModel(st_ema_model, avg_fn=ema_avg)

ema_loss_criterion = nn.CrossEntropyLoss()

ema_optimizer = torch.optim.SGD(st_model.parameters(), lr=learning_rate)

ema_n_total_steps = len(train_loader)

print(ema_n_total_steps)


def ema_student_train_method(num_epochs,data_loader,loss_criterion,optimizer,model):
    ema_n_correct=0
    # t_loss=0
    loss=0
    for i, (images, labels) in enumerate(data_loader):

        images = images.to(device)
        labels = labels.to(device)
        # Forward pass
        t_out = teacher_model(images)
        s_out = st_ema_model(images)
        _, predicted = torch.max(s_out, 1)
        t_loss = t_n_s_loss_fn(t_out,s_out, labels)
        loss += t_loss
        # Backward and optimize
        st_optimizer.zero_grad()
        t_loss.backward()
        ema_optimizer.step()
        ema_model.update_parameters(model)
        pred = ema_model(images)
        # print(pred.argmax(dim =1))
        ema_n_correct += (pred.argmax(dim=1) == labels).sum().item()
        # print(i)
        if (i+1) % 500 == 0:
            acc = (100.0 * ema_n_correct )/ (batch_size*(i+1))
            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{ema_n_total_steps}], Train Accuracy: {acc:.3f} , Train Loss: {t_loss.item():.4f}')
    loss /= len(data_loader)
    PATH = './cnn.pth'
    torch.save(model.state_dict(), PATH)
    return acc,loss

def ema_student_test_method(num_epochs,data_loader,model,loss_criterion):
    with torch.no_grad():
        n_correct = 0
        n_samples = 0
        loss=0
        for images, labels in data_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = ema_model(images)
            t_loss = loss_criterion(outputs, labels)
            loss +=t_loss
            # max returns (value ,index)
            print(torch.max(outputs, 1))
            _, predicted = torch.max(outputs, 1)
            n_samples += labels.size(0)
            n_correct += (predicted == labels).sum().item()
        acc = (100.0 * n_correct )/ (len(data_loader)*batch_size)
        final_loss=loss/len(data_loader)
        print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy of the network: {acc} %, Test Loss: {final_loss:.4f}')
        print('------------------------------------------------------------------------------------------------------------------------------------')
        return acc,final_loss


ema_train_loss_list = []
ema_test_loss_list = []
ema_train_accuracy_list = []
ema_test_accuracy_list = []
for epoch in range(num_epochs):
  train_acc,train_loss = ema_student_train_method(num_epochs,train_loader,st_loss_criterion,teacher_optimizer,st_model)
  test_acc, test_loss = ema_student_test_method(num_epochs,test_loader,st_model,st_loss_criterion)
  print(train_acc,train_loss, test_acc, test_loss)
  ema_train_loss_list.append(train_loss.item())
  ema_test_loss_list.append(test_loss.item())
  ema_train_accuracy_list.append(train_acc)
  ema_test_accuracy_list.append(test_acc)
print('Finished Training Student Network')

## loss vs epoch plot
def ema_loss_plot(train, test):
  plt.figure(figsize = (10,7))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.plot(range(len(train)),ema_train_loss_list, label = "Train Loss")
  plt.plot(range(len(test)),ema_test_loss_list, label = "Test Loss")
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.title("Loss")
  plt.legend()
  plt.show()
## Accuracy vs epoch plot
def ema_accuracy_plot(train, test):
  plt.figure(figsize = (10,7))
  # plt.plot(range(1, epoches*(len(train_dataloader)*BATCH_SIZE) + 1), train)
  plt.plot(range(len(train)),ema_train_accuracy_list, label = "Train Accuracy")
  plt.plot(range(len(test)),ema_test_accuracy_list, label = "Test Accuracy")
  plt.xlabel("Epochs")
  plt.ylabel("Accuracy")
  plt.title("Accuracy")
  plt.legend()
  plt.show()

# loss_plot(student_train_loss_list, student_test_loss_list)
print('  ')
ema_accuracy_plot(ema_train_accuracy_list, ema_test_accuracy_list)









