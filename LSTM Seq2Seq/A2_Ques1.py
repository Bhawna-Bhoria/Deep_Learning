# -*- coding: utf-8 -*-
"""Copy of DL_Assign2_Ques1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zZAzjGu-RrzrdNA25QtLJKRRMP71vkn0
"""

pip install torch==1.8.0

pip install torchtext==0.9.0

import torch
import torch.nn as nn
import torch.optim as optim
import torchtext
from torchtext.legacy.datasets import Multi30k
from torchtext.legacy.data import Field, BucketIterator,TabularDataset

import spacy
import numpy as np

import random
import math
import time

SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# import torch

# print(torch.__version__)

# spacy_de = spacy.load('de_core_news_sm')
# spacy_en = spacy.load('en_core_web_sm')
from torchtext.legacy.data import Field, TabularDataset
def tokenize_text(text):
    """
    Tokenizes German text from a string into a list of strings (tokens) and reverses it
    """
    return list(text)

SRC = Field(tokenize = tokenize_text, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)

TRG = Field(tokenize = tokenize_text, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)
# train_data = torchtext.legacy.data.TabularDataset(
#     path='/content/hi.translit.sampled.train.tsv',
#     format='tsv', fields=[('trg', SRC), ('src', TRG)]
# )
# valid_data = torchtext.legacy.data.TabularDataset(
#     path='/content/hi.translit.sampled.dev.tsv',
#     format='tsv', fields=[('trg', SRC), ('src', TRG)]
# )
# test_data = torchtext.legacy.data.TabularDataset(
#     path='/content/hi.translit.sampled.test.tsv',
#     format='tsv', fields=[('trg', SRC), ('src', TRG)]
# )

train_data, valid_data, test_data = torchtext.legacy.data.TabularDataset.splits(
    path='/content/',
    train='hi.translit.sampled.train.tsv',
    validation='hi.translit.sampled.dev.tsv',
    test='hi.translit.sampled.test.tsv',
    format='tsv',
    fields=[('trg', SRC),('src', TRG)]
)
print(type(train_data))
print(f"Number of training examples: {len(train_data.examples)}")
print(f"Number of validation examples: {len(valid_data.examples)}")
print(f"Number of testing examples: {len(test_data.examples)}")
print(vars(train_data.examples[0]))

SRC.build_vocab(train_data, min_freq = 3)
TRG.build_vocab(train_data, min_freq = 3)
print(f"Unique tokens in source hindi vocabulary: {len(SRC.vocab)}")
print(f"Unique tokens in target english vocabulary: {len(TRG.vocab)}")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

size_batch = 128

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data), 
    batch_size = size_batch, 
    device = device
    # sort_key = lambda x: len(x.src),
    # sort_within_batch=True
    )

SRC.build_vocab(train_data)
TRG.build_vocab(train_data)

class TraditionalEncoder(nn.Module):
    def __init__(self, input_dim, embed_dim, hidden_dim, hidden_layers, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.hidden_layers = hidden_layers
        self.embedding = nn.Embedding(input_dim, embed_dim)
        self.rnn = nn.LSTM(embed_dim, hidden_dim, hidden_layers, dropout = dropout, bidirectional=False)
        # self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        #src = [src len, batch size]
        embedded = self.dropout(self.embedding(src))
        #embedded = [src len, batch size, emb dim]
        #cell = [n layers * n directions, batch_size, hidden_dim]
        outputs, (hidden, cell) = self.rnn(embedded)
        #outputs = [src len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch_size, hidden_dim]
        return hidden, cell

class TraditionalDecoder(nn.Module):
    def __init__(self, output_dim, embed_dim, hidden_dim, hidden_layers, dropout):
        super().__init__()
        
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.hidden_layers = hidden_layers
        
        self.embedding = nn.Embedding(output_dim, embed_dim)
        
        self.rnn = nn.LSTM(embed_dim, hidden_dim, hidden_layers, dropout = dropout, bidirectional=False)
        # self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout = dropout)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, cell):
        input = input.unsqueeze(0)
        embedded = self.dropout(self.embedding(input))
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden, cell

class TraditionalSeq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hidden_dim == decoder.hidden_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.hidden_layers == decoder.hidden_layers, \
            "Encoder and decoder must have equal number of layers!"
        
    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time
        
        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        #last hidden state of the encoder is used as the initial hidden state of the decoder
        hidden, cell = self.encoder(src)
        
        #first input to the decoder is the <sos> tokens
        input = trg[0,:]
        
        for t in range(1, trg_len):
            
            #insert input token embedding, previous hidden and previous cell states
            #receive output tensor (predictions) and new hidden and cell states
            output, hidden, cell = self.decoder(input, hidden, cell)
            
            #place predictions in a tensor holding predictions for each token
            outputs[t] = output
            
            #decide if we are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio
            
            #get the highest predicted token from our predictions
            top1 = output.argmax(1) 
            
            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            input = trg[t] if teacher_force else top1
        
        return outputs

Input_dim = len(TRG.vocab)
Output_dim = len(SRC.vocab)
print(Input_dim,Output_dim)
emb_enc_dim = 16
emb_dec_dim = 16
hid_dim = 16
hid_layers = 1
enc_drpout = 0.5
dec_drpout = 0.5

traditionalenc = TraditionalEncoder(Input_dim, emb_enc_dim, hid_dim, hid_layers, enc_drpout)
traditionaldec = TraditionalDecoder(Output_dim, emb_dec_dim, hid_dim, hid_layers, dec_drpout)

traditionalmodel = TraditionalSeq2Seq(traditionalenc, traditionaldec, device).to(device)

from matplotlib import pyplot as plt

def weight_initialisation(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)


def train(model, dataloader, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    size = len(SRC.vocab)
    corltn_tbl = torch.zeros((size, size), dtype=torch.long)
    for i, batch in enumerate(dataloader):
        src = batch.src
        trg = batch.trg
        optimizer.zero_grad()
        output = model(src, trg)
        output_dim = output.shape[-1]
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        loss = criterion(output, trg)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        epoch_loss += loss.item()
        Y=output.argmax(1)
        for i, p in enumerate(Y):
            corltn_tbl[trg[i]][p] += 1
    if(epoch == num_epochs-1):
        print('Correlation Table for Traditional LSTM (16,1,1,16):')
        for i in range(size):
          row = "  ".join([f"{corltn_tbl[i][j]:5}" for j in range(size)])
          print(row)
    return epoch_loss / len(dataloader)

def plot_graph(loss_dict):
  train_loss=loss_dict["train_loss"]
  test_loss=loss_dict["test_loss"]

  epochs = range(len(train_loss))
  plot = plt.figure(figsize=(10,4))
  plt.plot(epochs,train_loss,color='blue',label='train-loss')
  plt.plot(epochs,test_loss,color='purple',label='test-loss')
  plt.legend()
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.show()
def evaluate(model, dataloader, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for i, batch in enumerate(dataloader):
            src = batch.src
            trg = batch.trg
            output = model(src, trg) #turn off teacher forcing
            output_dim = output.shape[-1]
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)
            loss = criterion(output, trg)
            epoch_loss += loss.item()
    return epoch_loss / len(dataloader)
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

traditionalmodel.apply(weight_initialisation)

print(f'The model has {count_parameters(traditionalmodel):,} trainable parameters')

optimizer = optim.Adam(traditionalmodel.parameters(),lr=0.001)
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)

def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

num_epochs = 10
CLIP = 1

loss_dict= {"train_loss": [], "test_loss": []}
for epoch in range(num_epochs):
    start_time = time.time()
    train_loss = train(traditionalmodel, train_iterator, optimizer, criterion, CLIP)
    test_loss = evaluate(traditionalmodel, train_iterator, criterion)
    # print(train_loss,test_loss)
    # print(valid_loss)
    end_time = time.time()
    train
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    loss_dict["train_loss"].append(train_loss)
    loss_dict["test_loss"].append(test_loss)
    # if valid_loss < best_valid_loss:
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} |Test Loss: {test_loss:.3f}')
plot_graph(loss_dict)

Input_dim = len(TRG.vocab)
Output_dim = len(SRC.vocab)
print(Input_dim,Output_dim)
emb_enc_dim = 64
emb_dec_dim = 64
hid_dim = 64
hid_layers = 3
enc_drpout = 0.5
dec_drpout = 0.5

traditionalenc2 = TraditionalEncoder(Input_dim, emb_enc_dim, hid_dim, hid_layers, enc_drpout)
traditionaldec2 = TraditionalDecoder(Output_dim, emb_dec_dim, hid_dim, hid_layers, dec_drpout)

traditionalmodel2 = TraditionalSeq2Seq(traditionalenc2, traditionaldec2, device).to(device)

traditionalmodel2.apply(weight_initialisation)

print(f'The model has {count_parameters(traditionalmodel2):,} trainable parameters')

optimizer = optim.Adam(traditionalmodel2.parameters(),lr=0.001)
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)

num_epochs = 10
CLIP = 1

loss_dict= {"train_loss": [], "test_loss": []}
for epoch in range(num_epochs):
    start_time = time.time()
    train_loss = train(traditionalmodel2, train_iterator, optimizer, criterion, CLIP)
    test_loss = evaluate(traditionalmodel2, train_iterator, criterion)
    # print(train_loss,test_loss)
    # print(valid_loss)
    end_time = time.time()
    train
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    loss_dict["train_loss"].append(train_loss)
    loss_dict["test_loss"].append(test_loss)
    # if valid_loss < best_valid_loss:
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} |Test Loss: {test_loss:.3f}')
plot_graph(loss_dict)

class AttentionEncoder(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):
    super().__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.dropout = nn.Dropout(p)
    self.embedding = nn.Embedding(input_size, embedding_size)
    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)

    self.fc_hidden_state = nn.Linear(hidden_size*2, hidden_size)
    self.fc_cell_state = nn.Linear(hidden_size*2, hidden_size)
    

  def forward(self, x):
    embedding = self.dropout(self.embedding(x))
    encoder_output, (hidden, cell) = self.lstm(embedding)
    hidden = self.fc_hidden_state(torch.cat((hidden[0:1], hidden[1:2]), dim=2))
    cell = self.fc_cell_state(torch.cat((cell[0:1], cell[1:2]), dim=2))
    return encoder_output, hidden, cell


class AttentionDecoder(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):
    super().__init__()
    self.hidden_size = hidden_size
    self.output_dim = output_size
    self.num_layers = num_layers
    self.dropout = nn.Dropout(p)
    self.embedding = nn.Embedding(output_size, embedding_size)
    self.lstm = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers)

    self.energy = nn.Linear(hidden_size*3, 1)
    self.softmax = nn.Softmax(dim=0)
    self.relu = nn.ReLU()


    self.fc = nn.Linear(hidden_size, output_size)

  def forward(self, x, encoder_output, hidden, cell):

    x = x.unsqueeze(0)
    embedding = self.dropout(self.embedding(x))
    sequence_length = encoder_output.shape[0]
    h_reshaped = hidden.repeat(sequence_length, 1, 1)
    energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_output), dim = 2)))
    attention = self.softmax(energy)
    attention = attention.permute(1, 2, 0)
    encoder_output = encoder_output.permute(1,0,2)
    context_vector = torch.bmm(attention, encoder_output).permute(1, 0, 2)
    lstm_input = torch.cat((context_vector, embedding), dim=2)
    decoder_output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))
    predict = self.fc(decoder_output.squeeze(0))
    # predict = predict.squeeze(0)

    return predict, hidden, cell

class AttentionSeq2Seq(nn.Module):
  def __init__(self, encoder, decoder):
    super().__init__()
    self.encoder = encoder
    self.decoder = decoder

  def forward(self, source, target, teacher_force_ratio = 0.5):
    batch_size = target.shape[1]
    target_len = target.shape[0]
    trg_vocab_size = self.decoder.output_dim
    outputs = torch.zeros(target_len, batch_size, trg_vocab_size).to(device)
    encoder_output, hidden, cell = self.encoder(source)
    x = target[0,:]
    for t in range(1, target_len):
      output, h, c = self.decoder(x, encoder_output, hidden, cell)
      outputs[t] = output
      best_guess = output.argmax(1)
      x = target[t] if random.random() < teacher_force_ratio else best_guess

    return outputs

Input_dim = len(TRG.vocab)
Output_dim = len(SRC.vocab)
print(Input_dim,Output_dim)
emb_enc_dim = 16
emb_dec_dim = 16
hid_dim = 16
hid_layers = 1
enc_drpout = 0.5
dec_drpout = 0.5

attnenc = AttentionEncoder(Input_dim, emb_enc_dim, hid_dim, hid_layers, enc_drpout)
attndec = AttentionDecoder(Output_dim, emb_dec_dim, hid_dim, Output_dim, hid_layers, dec_drpout)

Attentionmodel = AttentionSeq2Seq(attnenc, attndec).to(device)

Attentionmodel.apply(weight_initialisation)

print(f'The model has {count_parameters(Attentionmodel):,} trainable parameters')

optimizer = optim.Adam(Attentionmodel.parameters(),lr=0.001)
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)

num_epochs = 10
CLIP = 1

loss_dict= {"train_loss": [], "test_loss": []}
for epoch in range(num_epochs):
    start_time = time.time()
    train_loss = train(Attentionmodel, train_iterator, optimizer, criterion, CLIP)
    test_loss = evaluate(Attentionmodel, train_iterator, criterion)
    # print(train_loss,test_loss)
    # print(valid_loss)
    end_time = time.time()
    train
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    loss_dict["train_loss"].append(train_loss)
    loss_dict["test_loss"].append(test_loss)
    # if valid_loss < best_valid_loss:
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} |Test Loss: {test_loss:.3f}')
plot_graph(loss_dict)

Input_dim = len(TRG.vocab)
Output_dim = len(SRC.vocab)
print(Input_dim,Output_dim)
emb_enc_dim = 64
emb_dec_dim = 64
hid_dim = 64
hid_layers = 1
enc_drpout = 0.5
dec_drpout = 0.5

attnenc2 = AttentionEncoder(Input_dim, emb_enc_dim, hid_dim, hid_layers, enc_drpout)
attndec2 = AttentionDecoder(Output_dim, emb_dec_dim, hid_dim, Output_dim, hid_layers, dec_drpout)

Attentionmodel2 = AttentionSeq2Seq(attnenc2, attndec2).to(device)

Attentionmodel2.apply(weight_initialisation)

print(f'The model has {count_parameters(Attentionmodel2):,} trainable parameters')

optimizer = optim.Adam(Attentionmodel2.parameters(),lr=0.001)
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)

num_epochs = 10
CLIP = 1

loss_dict= {"train_loss": [], "test_loss": []}
for epoch in range(num_epochs):
    start_time = time.time()
    train_loss = train(Attentionmodel2, train_iterator, optimizer, criterion, CLIP)
    test_loss = evaluate(Attentionmodel2, train_iterator, criterion)
    # print(train_loss,test_loss)
    # print(valid_loss)
    end_time = time.time()
    train
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    loss_dict["train_loss"].append(train_loss)
    loss_dict["test_loss"].append(test_loss)
    # if valid_loss < best_valid_loss:
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} |Test Loss: {test_loss:.3f}')
plot_graph(loss_dict)