# -*- coding: utf-8 -*-
"""M22MA003_PA5_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c8BHtDZtCN7W_HHyTudu29Lnq_SzZSXY
"""

#Loding pickle

!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl

!git clone https://github.com/NVlabs/stylegan2-ada-pytorch

!ls

# Commented out IPython magic to ensure Python compatibility.
# %cd stylegan2-ada-pytorch

!ls

!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git
!pip install ninja

import sys
sys.path.insert(0, "/content/stylegan2-ada-pytorch")

!wget http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2
!bzip2 -d shape_predictor_5_face_landmarks.dat.bz2

!pip install dnnlib

!pip install legacy

from __future__ import print_function
#%matplotlib inline
import os
import random
import torch
import matplotlib.animation as animation
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.backends.cudnn as cudnn
import torch.optim as optim
import pickle
import cv2
# from google.colab import drive
# drive.mount('/content/drive')
import torchvision.datasets as td
import torch.nn.parallel
import torch.nn.functional as F
import torchvision.utils as vutils
import numpy as np
import matplotlib.pyplot as plt
import torch.backends.cudnn as cudnn
import torch.utils.data
import torch.nn.init as init
import torchvision.transforms as transforms
from IPython.display import HTML
from torchvision.io import read_image
import torchvision.models as models
from PIL import Image
import dlib
import sys
from matplotlib import pyplot as plt
import dnnlib
import legacy

import imageio
from tqdm.notebook import tqdm
from google.colab import files

val_seed = 999
random.seed(val_seed)
torch.manual_seed(val_seed)
pkl_model = pickle.load(open('/content/ffhq.pkl', 'rb'))
print(pkl_model)

!python3 generate.py --outdir=out --trunc=1 --seeds=90,108,250,483,583,458,384,900,999,27 --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl

image_path = "/content/stylegan2-ada-pytorch/out/seed0999.png" 
image = Image.open(image_path)

plt.imshow(image)
plt.axis('off')
plt.show()

def find_eyes(image):
  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
  rectangles = detector(gray_image, 0)

  shape = predictor(gray_image, rectangles[0])
  eye_features = []

  for i in range(0, 5):
    eye_features.append((i, (shape.part(i).x, shape.part(i).y)))

  return (int(eye_features[3][1][0] + eye_features[2][1][0]) // 2, \
    int(eye_features[3][1][1] + eye_features[2][1][1]) // 2), \
    (int(eye_features[1][1][0] + eye_features[0][1][0]) // 2, \
    int(eye_features[1][1][1] + eye_features[0][1][1]) // 2)

def crop_stylegan(input_image):
  # Find eyes in the input image
  left_eye, right_eye = find_eyes(input_image)
  
  # Calculate the size of the face
  face_width = abs(right_eye[0] - left_eye[0])
  zoom_factor = 255/face_width
  
  # Consider the aspect ratio
  aspect_ratio = input_image.shape[0]/input_image.shape[1]
  new_width = input_image.shape[1] * zoom_factor
  
  # Resize the image with the new width and aspect ratio
  resized_img = cv2.resize(input_image, (int(new_width), int(new_width*aspect_ratio)))
  
  # Add border to the resized image
  border_size = 1024
  bordered_img = cv2.copyMakeBorder(
      resized_img,
      top=border_size,
      bottom=border_size,
      left=border_size,
      right=border_size,
      borderType=cv2.BORDER_REPLICATE)

  # Find eyes in the bordered image
  left_eye_bordered, right_eye_bordered = find_eyes(bordered_img)

  # Adjust to the offset used by StyleGAN2
  crop1 = left_eye_bordered[0] - 385 
  crop0 = left_eye_bordered[1] - 490
  
  # Return the cropped image
  return bordered_img[crop0:crop0+1024, crop1:crop1+1024]

n_link = "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
val_steps = 150
val_fps = 30
val_frz_steps = 30

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_5_face_landmarks.dat')

uploaded = files.upload()

if len(uploaded) != 1:
  print("Upload exactly 1 file for source.")
else:
  for k, v in uploaded.items():
    _, ext = os.path.splitext(k)
    os.remove(k)
    SOURCE_NAME = f"source{ext}"
    open(SOURCE_NAME, 'wb').write(v)



image_source = cv2.imread(SOURCE_NAME)
cropped_source = crop_stylegan(image_source)

cmd = f"python /content/stylegan2-ada-pytorch/projector.py --save-video 0 --num-steps 1000 --outdir=out_source --target=cropped_source.png --network={n_link}"
!{cmd}

uploaded = files.upload()

if len(uploaded) != 1:
  print("Upload exactly 1 file for target.")
else:
  for k, v in uploaded.items():
    _, ext = os.path.splitext(k)
    os.remove(k)
    TARGET_NAME = f"target{ext}"
    open(TARGET_NAME, 'wb').write(v)

image_target = cv2.imread(TARGET_NAME)

cropped_target = crop_stylegan(image_target)

img = cv2.cvtColor(cropped_source, cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.title('source')
plt.show()

img = cv2.cvtColor(cropped_target, cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.title('target')
plt.show()

cv2.imwrite("cropped_source.png", cropped_source)
cv2.imwrite("cropped_target.png", cropped_target)

cmd = f"python /content/stylegan2-ada-pytorch/projector.py --save-video 0 --num-steps 1000 --outdir=out_target --target=cropped_target.png --network={n_link}"
!{cmd}

lvec1 = np.load('/content/stylegan2-ada-pytorch/out_source/projected_w.npz')['w']
lvec2 = np.load('/content/stylegan2-ada-pytorch/out_target/projected_w.npz')['w']

network_pkl = "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
device = torch.device('cuda')
with dnnlib.util.open_url(network_pkl) as fp:
    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore

diff = lvec2 - lvec1
step = diff / val_steps
current = lvec1.copy()
target_uint8 = np.array([1024,1024,3], dtype=np.uint8)

video = imageio.get_writer(f'/content/movie.mp4', mode='I', fps=val_fps, codec='libx264', bitrate='16M')

for j in tqdm(range(val_steps)):
  z = torch.from_numpy(current).to(device)
  synth_image = G.synthesis(z, noise_mode='const')
  synth_image = (synth_image + 1) * (255/2)
  synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()

  repeat = val_frz_steps if j==0 or j==(val_steps-1) else 1
  
  for i in range(repeat):
    video.append_data(synth_image)
  current = current + step


video.close()